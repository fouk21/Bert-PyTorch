{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34de88b9",
   "metadata": {},
   "source": [
    "<h1>AI2 - PROJECT4 - FOUKANELIS CHRISTOS GEORGIOS - 1115201900204<h1>\n",
    "BERT FINE TUNING FOR IMDB SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74259ee",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2da49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c1a19",
   "metadata": {},
   "source": [
    "INITIALIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eaa8308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  1% |  4% |\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "gpu_usage()\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "cuda.select_device(0)\n",
    "print(torch.cuda.memory_allocated(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382062b2",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7572ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "def remove_links(x):\n",
    "    return re.sub(r\"http\\S+\", \"\", x)\n",
    "\n",
    "def remove_punctuation(x):\n",
    "    return re.sub(r'[^\\w\\s]', '', x)\n",
    "\n",
    "def lowercase(x):\n",
    "    return x.lower()\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    return ' '.join([word for word in x.split() if word not in cachedStopWords])\n",
    "\n",
    "def remove_html(x):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', x)\n",
    "\n",
    "def sentiment_label(x):\n",
    "    if x >= 7.0:\n",
    "        return 1\n",
    "    elif x <= 4.0:\n",
    "        return 0\n",
    "    \n",
    "def word_count(x):\n",
    "    return len(x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef80d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.imdb.com/title/tt0120623/usercomments</td>\n",
       "      <td>10.0</td>\n",
       "      <td>I thought this was a quiet good movie. It was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.imdb.com/title/tt0043117/usercomments</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Wagon Master is a very unique film amongst Joh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.imdb.com/title/tt0043117/usercomments</td>\n",
       "      <td>10.0</td>\n",
       "      <td>This film has to be as near to perfect a film ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.imdb.com/title/tt0835204/usercomments</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I gave this 4 stars because it has a lot of in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.imdb.com/title/tt0499603/usercomments</td>\n",
       "      <td>10.0</td>\n",
       "      <td>This movie is really genuine and random. It's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                url  rating  \\\n",
       "0  http://www.imdb.com/title/tt0120623/usercomments    10.0   \n",
       "1  http://www.imdb.com/title/tt0043117/usercomments     9.0   \n",
       "2  http://www.imdb.com/title/tt0043117/usercomments    10.0   \n",
       "3  http://www.imdb.com/title/tt0835204/usercomments     4.0   \n",
       "4  http://www.imdb.com/title/tt0499603/usercomments    10.0   \n",
       "\n",
       "                                              review  \n",
       "0  I thought this was a quiet good movie. It was ...  \n",
       "1  Wagon Master is a very unique film amongst Joh...  \n",
       "2  This film has to be as near to perfect a film ...  \n",
       "3  I gave this 4 stars because it has a lot of in...  \n",
       "4  This movie is really genuine and random. It's ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('imdb-reviews.csv',sep = '\\t', engine = 'python')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111e3ab",
   "metadata": {},
   "source": [
    "DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a81c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "df['review'] = df['review'].apply(remove_links)\n",
    "df['review'] = df['review'].apply(lowercase)\n",
    "df['review'] = df['review'].apply(remove_punctuation)\n",
    "df['review'] = df['review'].apply(remove_stopwords)\n",
    "df['review'] = df['review'].apply(remove_html)\n",
    "df['sentiment'] = df['rating'].apply(sentiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2ef0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['review']\n",
    "df['label'] =df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1552b767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.imdb.com/title/tt0120623/usercomments</td>\n",
       "      <td>10.0</td>\n",
       "      <td>thought quiet good movie fun watch liked best ...</td>\n",
       "      <td>1</td>\n",
       "      <td>thought quiet good movie fun watch liked best ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.imdb.com/title/tt0043117/usercomments</td>\n",
       "      <td>9.0</td>\n",
       "      <td>wagon master unique film amongst john fords wo...</td>\n",
       "      <td>1</td>\n",
       "      <td>wagon master unique film amongst john fords wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.imdb.com/title/tt0043117/usercomments</td>\n",
       "      <td>10.0</td>\n",
       "      <td>film near perfect film john ford made film mag...</td>\n",
       "      <td>1</td>\n",
       "      <td>film near perfect film john ford made film mag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.imdb.com/title/tt0835204/usercomments</td>\n",
       "      <td>4.0</td>\n",
       "      <td>gave 4 stars lot interesting themes many alrea...</td>\n",
       "      <td>0</td>\n",
       "      <td>gave 4 stars lot interesting themes many alrea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.imdb.com/title/tt0499603/usercomments</td>\n",
       "      <td>10.0</td>\n",
       "      <td>movie really genuine random really hard find m...</td>\n",
       "      <td>1</td>\n",
       "      <td>movie really genuine random really hard find m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                url  rating  \\\n",
       "0  http://www.imdb.com/title/tt0120623/usercomments    10.0   \n",
       "1  http://www.imdb.com/title/tt0043117/usercomments     9.0   \n",
       "2  http://www.imdb.com/title/tt0043117/usercomments    10.0   \n",
       "3  http://www.imdb.com/title/tt0835204/usercomments     4.0   \n",
       "4  http://www.imdb.com/title/tt0499603/usercomments    10.0   \n",
       "\n",
       "                                              review  sentiment  \\\n",
       "0  thought quiet good movie fun watch liked best ...          1   \n",
       "1  wagon master unique film amongst john fords wo...          1   \n",
       "2  film near perfect film john ford made film mag...          1   \n",
       "3  gave 4 stars lot interesting themes many alrea...          0   \n",
       "4  movie really genuine random really hard find m...          1   \n",
       "\n",
       "                                                text  label  \n",
       "0  thought quiet good movie fun watch liked best ...      1  \n",
       "1  wagon master unique film amongst john fords wo...      1  \n",
       "2  film near perfect film john ford made film mag...      1  \n",
       "3  gave 4 stars lot interesting themes many alrea...      0  \n",
       "4  movie really genuine random really hard find m...      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a405c22",
   "metadata": {},
   "source": [
    "DATA SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3c116e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8bcc1f",
   "metadata": {},
   "source": [
    "BERT EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c24085f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ab43464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b670c597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\anaconda3\\envs\\ai\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd378e4e",
   "metadata": {},
   "source": [
    "TRAIN AND VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22fbf553",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c7a2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 64\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e36cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3044f",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36bff56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,384)\n",
    "      \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(384,1)\n",
    "\n",
    "        #sigmoid activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask).to_tuple()\n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "      \n",
    "        # apply sigmoid activation\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62aca8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581d9714",
   "metadata": {},
   "source": [
    "MODEL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d0d3474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\anaconda3\\envs\\ai\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 0.001)          # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1cd00306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "cross_entropy  = nn.BCELoss() #weight=weights\n",
    "\n",
    "# number of training epochs\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224d3f8d",
   "metadata": {},
   "source": [
    "TRAINING AND EVALUATION OF MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74267517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "        \n",
    "        preds = torch.flatten(preds)\n",
    "\n",
    "        preds = preds.type(torch.DoubleTensor)\n",
    "        labels = labels.type(torch.DoubleTensor)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb191d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "        # model predictions\n",
    "        preds = model(sent_id, mask)\n",
    "        preds = torch.flatten(preds)\n",
    "        \n",
    "        preds = preds.type(torch.DoubleTensor)\n",
    "        labels = labels.type(torch.DoubleTensor)\n",
    "\n",
    "        # compute the validation loss between actual and predicted values\n",
    "        loss = cross_entropy(preds,labels)\n",
    "\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "968cc664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.609\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 2 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.609\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 3 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.607\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 4 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.610\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 5 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.604\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 6 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.603\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 7 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.606\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 8 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.604\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 9 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.602\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 10 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.604\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 11 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.603\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 12 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.601\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 13 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.603\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 14 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.601\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 15 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.603\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 16 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.603\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 17 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.603\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 18 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.602\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 19 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.599\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 20 / 20\n",
      "  Batch    50  of    493.\n",
      "  Batch   100  of    493.\n",
      "  Batch   150  of    493.\n",
      "  Batch   200  of    493.\n",
      "  Batch   250  of    493.\n",
      "  Batch   300  of    493.\n",
      "  Batch   350  of    493.\n",
      "  Batch   400  of    493.\n",
      "  Batch   450  of    493.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    106.\n",
      "  Batch   100  of    106.\n",
      "\n",
      "Training Loss: 0.601\n",
      "Validation Loss: 0.005\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf2ca2",
   "metadata": {},
   "source": [
    "TESTING PREFORMANCE OF MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1538065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e7f8a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = torch.round(preds)\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "72ddec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3375\n",
      "           1       0.72      0.61      0.66      3377\n",
      "\n",
      "    accuracy                           0.69      6752\n",
      "   macro avg       0.69      0.69      0.69      6752\n",
      "weighted avg       0.69      0.69      0.69      6752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc38a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################TESTING-INSERT CSV FILE HERE###########################################################\n",
    "\n",
    "# insert csv path here\n",
    "test_csv_path = ''\n",
    "test_df = pd.read_csv(test_csv_path, sep = '\\t', engine = 'python')\n",
    "# clean the data\n",
    "test_df['review'] = test_df['review'].apply(remove_links)\n",
    "test_df['review'] = test_df['review'].apply(lowercase)\n",
    "test_df['review'] = test_df['review'].apply(remove_punctuation)\n",
    "test_df['review'] = test_df['review'].apply(remove_stopwords)\n",
    "test_df['review'] = test_df['review'].apply(remove_html)\n",
    "test_df['sentiment'] = test_df['rating'].apply(sentiment_label)\n",
    "test_df['text'] = test_df['review']\n",
    "test_df['label'] = test_df['sentiment']\n",
    "\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_df['text'].tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_df['labels'].tolist())\n",
    "\n",
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = torch.round(preds)\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "print(classification_report(test_y, preds))\n",
    "###################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
